{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "\n",
    "# 模型融合方法\n",
    "本节主要介绍三种方法：袋装Bagging、提升Boosting、随机森林。\n",
    "\n",
    "## 简介：\n",
    "三种方法的基本思想：将多个基模型集成为一个模型，目的就是创造一个符合的分类器。好处在哪儿？若现在有5个分类器，当且仅当有两个以上的分类器同时出错的时候，复合分类器才会出错。每个分类器还可以分配到不同的线程上，因此可以并行。\n",
    "\n",
    "![](images/vote.png)\n",
    "\n",
    "为什么会起作用？\n",
    "\n",
    "左图为基模型，右图为集成模型。\n",
    "![](images/ensmble_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging 袋装\n",
    "\n",
    "假设现在有一个样本空间，然后使用一个模型来训练，但是无法确信现在这个模型是否能够有良好的效果。于是对样本空间放回抽样m次，来训练m个模型，这m个模型共同投票，来对样本进行输出，确保最高的正确率。\n",
    "\n",
    "算法框架如下：\n",
    "![](images/Bagging.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics,ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "\n",
    "data = datasets.load_wine()\n",
    "label = data['target']\n",
    "train = data['data']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,Y_train,Y_val = train_test_split(train,label,test_size = 0.2,random_state = 0, shuffle = True)\n",
    "\n",
    "bagging_model = ensemble.BaggingClassifier(base_estimator=linear_model.RidgeClassifier(),n_estimators=10, random_state=0)\n",
    "bagging_model.fit(X_train,Y_train)\n",
    "\n",
    "print(metrics.precision_score(Y_val, bagging_model.predict(X_val), average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提升和AdaBoost\n",
    "\n",
    "给定数据集$D$，$AdaBoost$对每个训练样本赋予$1/d$的相同权重。若有$k$个分类器，则执行$k$次。每次不正确的分类，则权重增加，若元组正确分类，则它的权重减少，该算法的主要思想是：当建立分类器的时候，希望下一轮的时候分类器能够更加关注分类错位的样本，从而提高“难以分类”的样本的正确率。从而建立了一个互补的分类器。\n",
    "\n",
    "![](images/adaboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**两种算法的区别：AdaBoost更加关注错误分类元组，从而让复合模型出于以过拟合的风险，但是Bagging就不会出现这个问题。与单个模型相比，两者都能够显著提高准确率，但是Ada会更高一点。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912280701754386\n"
     ]
    }
   ],
   "source": [
    "ada_model = ensemble.AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "ada_model.fit(X_train,Y_train)\n",
    "\n",
    "print(metrics.precision_score(Y_val, ada_model.predict(X_val), average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林\n",
    "\n",
    "基于Bagging的思想与树模型结合。即森林中的每一棵树都是决策树，决策树的集合就是森林。更准确的说，每一棵树都依赖于独立抽样。分类时，每棵树都投票并且返回票数最多的一类。\n",
    "\n",
    "随机森林的准确率可以和AdaBoost相媲美，对错误和离群点更棒。随着随机森林树的增加，森林的泛化误差收敛。故，很难过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777779\n"
     ]
    }
   ],
   "source": [
    "rf_model = ensemble.RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "rf_model.fit(X_train,Y_train)\n",
    "\n",
    "print(metrics.precision_score(Y_val, rf_model.predict(X_val), average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提高不平衡数据的分类准确率，只针对分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设原训练集包含100个正、1000个负。\n",
    "\n",
    "**过抽样**：将负的训练样本复制。形成包含1000个正、1000个负的新训练集。\n",
    "\n",
    "**欠抽样**：将正的训练样本复制。形成包含100个正、100个负的新训练集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ImBalance](https://imbalanced-learn.readthedocs.io/en/stable/api.html)\n",
    "\n",
    "该网站提供处理不平衡数据的库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype selection\n",
    "under_sampling.CondensedNearestNeighbour([…])\tClass to perform under-sampling based on the condensed nearest neighbour method.\n",
    "\n",
    "under_sampling.EditedNearestNeighbours([…])\tClass to perform under-sampling based on the edited nearest neighbour method.\n",
    "\n",
    "under_sampling.RepeatedEditedNearestNeighbours([…])\tClass to perform under-sampling based on the repeated edited nearest neighbour method.\n",
    "\n",
    "under_sampling.AllKNN([sampling_strategy, …])\tClass to perform under-sampling based on the AllKNN method.\n",
    "\n",
    "under_sampling.InstanceHardnessThreshold([…])\tClass to perform under-sampling based on the instance hardness threshold.\n",
    "\n",
    "under_sampling.NearMiss([sampling_strategy, …])\tClass to perform under-sampling based on NearMiss methods.\n",
    "\n",
    "under_sampling.NeighbourhoodCleaningRule([…])\tClass performing under-sampling based on the neighbourhood cleaning rule.\n",
    "\n",
    "under_sampling.OneSidedSelection([…])\tClass to perform under-sampling based on one-sided selection method.\n",
    "\n",
    "under_sampling.RandomUnderSampler([…])\tClass to perform random under-sampling.\n",
    "\n",
    "under_sampling.TomekLinks([…])\tClass to perform under-sampling by removing Tomek’s links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling methods\n",
    "over_sampling.ADASYN([sampling_strategy, …])\tPerform over-sampling using Adaptive Synthetic (ADASYN) sampling approach for imbalanced datasets.\n",
    "\n",
    "over_sampling.BorderlineSMOTE([…])\tOver-sampling using Borderline SMOTE.\n",
    "\n",
    "over_sampling.KMeansSMOTE([…])\tApply a KMeans clustering before to over-sample using SMOTE.\n",
    "\n",
    "over_sampling.RandomOverSampler([…])\tClass to perform random over-sampling.\n",
    "\n",
    "over_sampling.SMOTE([sampling_strategy, …])\tClass to perform over-sampling using SMOTE.\n",
    "\n",
    "over_sampling.SMOTENC(categorical_features)\tSynthetic Minority Over-sampling Technique for Nominal and Continuous (SMOTE-NC).\n",
    "\n",
    "over_sampling.SVMSMOTE([sampling_strategy, …])\tOver-sampling using SVM-SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of over- and under-sampling methods\n",
    "combine.SMOTEENN([sampling_strategy, …])\tClass to perform over-sampling using SMOTE and cleaning using ENN.\n",
    "\n",
    "combine.SMOTETomek([sampling_strategy, …])\tClass to perform over-sampling using SMOTE and cleaning using Tomek links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "ensemble.BalanceCascade(**kwargs)\tCreate an ensemble of balanced sets by iteratively under-sampling the imbalanced dataset using an estimator.\n",
    "\n",
    "ensemble.BalancedBaggingClassifier([…])\tA Bagging classifier with additional balancing.\n",
    "\n",
    "ensemble.BalancedRandomForestClassifier([…])\tA balanced random forest classifier.\n",
    "\n",
    "ensemble.EasyEnsemble(**kwargs)\tCreate an ensemble sets by iteratively applying random under-sampling.\n",
    "\n",
    "ensemble.EasyEnsembleClassifier([…])\tBag of balanced boosted learners also known as EasyEnsemble.\n",
    "\n",
    "ensemble.RUSBoostClassifier([…])\tRandom under-sampling integrating in the learning of an AdaBoost classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
